\chapter{Related Work}\label{chapter:related_work}
% overview of different policy optimization problems
In this chapter, we discuss related work on continuous action space reinforcement learning problems which use neural networks as function approximators.

\section{Continuous Q-learning}
Q-learning has been very successful in discrete action space problems. In Q-learning, the optimal policy is found by learning an action-value function $Q(s,a)$. At each time step $t$, the algorithm interacts with environment by taking action $a_t$ at states $s_t$ observing next state $s_{t+1}$ and reward $r(s_t, a_t)$. The Q-values for state-action pair $(s_t, a_t)$ are then updated by the following equation

\begin{equation}\label{eq:qlearning}
Q(s_t,a_t) = (1-\alpha) Q(s_t,a_t) + \alpha [r(s_t,a_t) + \gamma \max_a Q(s_{t+1}, a_r)],
\end{equation}

where $\alpha \in (0, 1]$ is the learning rate of the algorithm.


The maximization $\max_a Q(s_{t+1}, a_t)$ is easy to calculate when actions are discrete as there are finite Q-values at each state. However, it becomes intractable when actions are continuous. 

\subsection{Normalized Advantage Function}
Normalized advantage function (NAF) \cite{gu2016} addresses the problem of estimating $\max_a Q(s_{t+1}, a_t)$ for continuous actions. It represents the Q-function such that the maximization can be analytically calculated from the state-value function $V(s_t)$ and advantage term $A(s_t, a_t)$. The state-value function and advantage are estimated by a neural network. The Q-function is calculated as 

\begin{equation}
Q(s_t, a_t) = A(s_t, a_t|\theta^A) + V(s_t|\theta^V),
\end{equation}

\begin{equation}\label{eq:naf}
A(s_t, a_t|\theta^A) = -\dfrac{1}{2} (a_t - \mu(s_t|\theta_\mu))^T  P(s_t|\theta^P) (a_t - \mu(s_t|\theta_\mu)),
\end{equation}

where $\mu$ is the deterministic policy used to generate the experience. $P(s_t|\theta^P)$ and $V(s_t|\theta^V)$ are estimated by the neural network. Eqn. \ref{eq:naf} is quadratic in $\mu$, so $\mu$ maximizes the Q-function.



\section{Policy Gradient Methods}
Policy gradient algorithms directly optimize the policy parameters by estimating the gradient of the policy's performance with respect to the policy parameters. The policy is represented as a parametrized probability distribution $\pi [a|s, \theta]$, where $\theta$ is the policy parameter vector, represented by a neural network. The policy performance is usually measured by the expected value of future rewards and therefore the goal of a policy gradient algorithm is the following optimization problem
\begin{equation} \label{03:eq_pg}
    \max_\theta \mathbb{E}[R|\pi_\theta].
\end{equation}


\noindent
This optimization problem can be solved by gradient ascent on the gradient of $\mathbb{E}[R|\pi_\theta]$ with respect to the policy parameters $\theta$. To find the policy gradient, we use the following equality which finds the gradient of expectation $\mathbb{E}_{x \sim p(x|\theta)}[f(x)]$ with respect to distribution parameters $\theta$,

\begin{equation} \label{03:eq_pgrad}
\nabla_\theta \mathbb{E}_x[f(x)] = \mathbb{E}_x[\nabla_\theta \log p(x|\theta) f(x)],
\end{equation}


where $f(x)$ is a scalar valued function and $x$ is sampled from the probability distribution $p(x|\theta)$. The proof of this identity is given in \cite{schulman_policy_opt}. We can approximate the gradient of expectation by sampling $N$ values $x_1, x_2,\dots, x_N$ form $p(x|\theta)$ and taking the average

\begin{equation} \label{03:eq_est}
\nabla_\theta \mathbb{E}_x[f(x)] \approx \dfrac{1}{N}\sum_{n=1}^N \nabla_\theta \log p(x_i|\theta) f(x_i).
\end{equation}

\noindent
Now, to apply the above idea to optimize Eqn. \ref{03:eq_pg}, the policy should be \textit{stochastic}. Let us consider a trajectory $\tau$ generated by the policy $\pi(a|s,\theta)$.

\begin{equation}
\nonumber
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_{T-1}, a_{T-1}, r_{T-1}, s_{T}).
\end{equation}

Let probability distribution of the trajectory be $p(\tau|\theta)$, parameterized by $\theta$. $p(\tau|\theta)$ can be expanded in terms of $\pi(a|s,\theta)$, transition probability distribution $p(s_{t+1}| s_t, a_t)$ and the initial state distribution $\mu(s)$ as

\begin{align} \label{03:eq_trajectory}
\nonumber
p(\tau|\theta) = \mu(s_0) & \pi(a_0|s_0, \theta) p(s_1|s_0, a_0) \\& \pi(a_1|s_1, \theta) \dots \pi(a_{T-1}|s_{T-1}, \theta) p(s_T|s_{T-1},a_{T-1}).
\end{align}

Taking logarithm of Eqn. \ref{03:eq_trajectory} and differentiating with respect to $\theta$, the transition probability terms $p(s_{t+1}| s_t, a_t)$ and initial state distribution $\mu(s_0)$ will become zero as these are independent of $\theta$. Therefore, we get

\begin{equation}
\nonumber
\nabla_\theta \log p(\tau|\theta) = \sum_{t=0}^{T-1}\log \pi(a_t|s_t,\theta).
\end{equation}

Using Eqn. \ref{03:eq_pgrad} the policy gradient for the expected reward with respect to $\theta$ on experience $\tau$ becomes 

\begin{equation}\label{03:eq_policygradient}
\nabla_\theta \mathbb{E}_\tau[R(\tau)] = \mathbb{E}_\tau \left [ \sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t| s_t, \theta) R(\tau) \right].
\end{equation}

There are several formulations to calculate $R(\tau)$. The most common formulation uses an advantage estimate instead of $R(\tau)$ which is given as

\begin{equation} \label{03:eq_adv}
A(s_t) = \hat{R}(\tau) - V(s_t|\theta^V),
\end{equation}

where $\hat{R}(\tau)$ is the discounted return for some experience $\tau$. $V(s_t|\theta^V)$ is the state-value function at $s_t$ which is represented by a critic network. The advantage estimate represents the increase in expected return by taking a particular action at state $s_t$. Here, the advantage signifies how good the actor network is at predicting an action at time step $t$ with respect to the critic network.

There exist a variety of policy gradient algorithms which use Eqn. \ref{03:eq_policygradient} to calculate policy gradients. We will discuss the most popular of these algorithms here.

\subsection{Deterministic Policy Gradients}
The deterministic policy gradients (DPG) \cite{silver2014deterministic} is an off-policy, actor-critic algorithm which uses deterministic policies. In the last section, we proved that a stochastic policy is required to calculate the policy gradients. However, Silver et al. proved that the policy gradients do exist for deterministic policies and are a limiting case of stochastic policy gradients.

Since the policy is deterministic, external process noise is added for exploration. In the original DPG paper, they use Ornstein-Uhlenbeck process noise \cite{ounoise}, but any other noise such as parameter noise \cite{paramnoise} could be used instead. DPG uses experience replay and soft updates to stabilize the training. 

\subsection{Stochastic Value Gradients}
Stochastic value gradients (SVG) proposed by \cite{svg} provides a family of reinforcement learning algorithms ranging from model-based to model-free. SVG(0) is the model-free variant of stochastic value gradients. SVG(0) is very similar to DDPG and uses a stochastic policy by parameterizing the policy with a noise variable. At each training step, the agent collects the experience and uses it to update the policy and value network.


\subsection{Trust Region Policy Optimization}
Trust region policy optimization (TRPO) \cite{schulman2015trust} makes vanilla policy gradient algorithms stable and sample efficient. Vanilla policy gradient algorithms use an estimator of the reward $R(\tau)$ (calculated using Eqn. \ref{eq:discreward} for $T$ steps) obtained from the generated trajectory $\tau$. Since, vanilla policy algorithms update policy from the sampled batch which is generated using the same policy, if some gradient update leads to a bad policy, the bad policy will then generate bad trajectories leading to further drop in the performance and it becomes hard for algorithm to recover.

TRPO addresses this problem by using a \textit{surrogate objective} which is given as 

\begin{equation}
L_{\pi_{old}}(\pi) = \dfrac{1}{N} \sum_{i=1}^N \dfrac{\pi(a_i|s_i)}{\pi_{old}(a_i|s_i)} \hat{A_i},
\end{equation}

where $\pi_{old}$ is the old policy to which the N samples were generated, $\pi$ is the current policy and $\hat{A_i}$ is the advantage estimate (same as $R(\tau)$) calculated on $\pi_{old}$. The policy gradient can be calculated by differentiating the surrogate objective loss. TRPO is an on-policy policy gradient algorithm.

\subsection{Asynchronous Advantage Actor Critic}
Asynchronous advantage actor-critic (A3C) \cite{a3c} is an on-policy, actor-critic algorithm. The policy gradient in A3C is calculated using Eqn. \ref{03:eq_policygradient} and Eqn. \ref{03:eq_adv}. A3C trains multiple actor-critic networks asynchronously by running multiple agent threads. Each agent thread uses its local copy of actor-critic network. The local network parameters are synchronized with a global network at the start of each iteration. Each agent accumulates local gradients and updates the global network at end of each training step.

The A3C agent uses entropy based exploration. A3C works well on many tasks because multiple actors make the updates independent and identically distributed which result in faster convergence of networks. A3C typically uses 4, 8 or 16 parallel threads.

\newpage
\section{Continuous Distribution Estimation}
Continuous probability distributions when represented using neural networks predict either a point estimate or parameters of a Gaussian distribution. This representation is limited when a rich representation of probability distribution is required. Two approaches to deal with this problem are described below.

\subsection{Mixture Density Networks}
Mixture density networks (MDN) \cite{bishop_mdn} predicts a mixture model as the output of the neural network. So, MDNs can represent any probability distribution completely. This is based on the observation that when estimating a continuous probability distribution using a neural network, only a point estimate can be produced which is the conditional average of target data. This point estimate provides a very limited description of the target variable.

\subsection{Multiple Hypothesis Prediction}
Multiple hypothesis prediction (MHP) \cite{rupprecht2017iccv} proposes a framework to train and reformulate single prediction neural networks as multiple output networks.  This formulation shows performance improvement in a variety of tasks such as image classification, pose estimation, semantic segmentation etc.